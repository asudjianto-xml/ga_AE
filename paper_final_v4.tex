\documentclass[11pt]{article}

% =========================
% Packages
% =========================
\usepackage[a4paper,margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage{microtype}

\usepackage{amsmath,amssymb,amsthm,bm}
\usepackage{mathtools}

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{xcolor}

\usepackage{hyperref}
\usepackage{cleveref}

\usepackage{algorithm}
\usepackage{algpseudocode}

\usepackage{tikz}
\usetikzlibrary{arrows.meta,calc,positioning}

% =========================
% Hyperref setup
% =========================
\hypersetup{
  colorlinks=true,
  linkcolor=blue!60!black,
  citecolor=blue!60!black,
  urlcolor=blue!60!black
}

% =========================
% Theorem environments
% =========================
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{remark}{Remark}

% =========================
% Macros
% =========================
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\norm}[1]{\left\lVert #1\right\rVert}
\DeclareMathOperator{\Tr}{Tr}
\DeclareMathOperator{\KL}{KL}
\DeclareMathOperator{\logdet}{logdet}
\newcommand{\eps}{\varepsilon}
\newcommand{\onman}{\textsc{on-manifold}}
\newcommand{\offman}{\textsc{off-manifold}}

\title{Escaping the Autoencoder Trap: Grassmannian Tangent-Space Regularization for Tail Coverage}
\author{Agus Sudjianto \\
H2O.ai, \texttt{agus.sudjianto@h2o.ai} \\
Center for Trustworthy AI Through Model Risk Management, \\
University of North Carolina Charlotte \\}
\date{December 2025}

\begin{document}
\maketitle

% =========================================================
\begin{abstract}
Autoencoders achieve low reconstruction error but often produce poor-quality samples from a chosen latent prior (e.g., $\mathcal{N}(0,I)$), particularly failing to capture rare or tail modes. This \emph{generative gap} arises from a fundamental asymmetry: reconstruction operates \onman\ (conditioned on real data), while generation operates \offman\ (sampling from the prior). We formalize this failure as geometric degeneracy in the learned encoder-decoder maps, specifically the collapse of local tangent space structure. We introduce Jacobian-based diagnostics that measure local volume and $k$-subspace collapse, revealing that standard density-based regularization (e.g., KL divergence in VAEs) can exhibit severe tail mass misallocation on certain datasets. As an alternative, we propose \textbf{geometric regularization} using two novel terms derived from exterior algebra: (1) \textbf{Grassmann spread loss} that repels tangent $k$-blades on the Grassmann manifold, and (2) \textbf{blade entropy loss} that maximizes diversity across multi-grade volumes. On mixture-of-Gaussians benchmarks with rare tail modes, our deterministic geometrically-regularized autoencoder substantially improves tail coverage compared to contractive autoencoders, while avoiding the tail mass misallocation observed in standard VAEs. On MNIST with class imbalance, our method demonstrates substantially improved calibration and sample diversity compared to VAE baselines, which exhibit severe mode collapse. Our framework provides a geometry-first alternative to density-based priors for synthetic data generation tasks requiring robust tail coverage.
\end{abstract}

% =========================================================
\section{Introduction}

Autoencoders (AEs) and variational autoencoders (VAEs) \cite{KingmaWelling2014} are widely used for learning compressed representations and generating synthetic data. Their appeal stems from training stability and the ability to achieve low reconstruction error. However, good reconstruction does not guarantee good generation: a model can perfectly reconstruct training data while producing meaningless samples from the latent prior. We call this phenomenon the \textbf{autoencoder trap}.

The root cause is a regime mismatch:
\begin{itemize}[leftmargin=*,topsep=4pt]
\item \textbf{\onman\ regime:} Reconstruction $\hat{x} = D(E(x))$ with $x \sim p_{\text{data}}$
\item \textbf{\offman\ regime:} Generation $x^{\text{gen}} = D(z)$ with $z \sim p(z)$
\end{itemize}

Standard training objectives optimize reconstruction loss on the \onman\ regime but provide no geometric guarantees \offman. The decoder $D$ is only supervised along the encoder image $E(p_{\text{data}})$, leaving its behavior elsewhere unconstrained.

\subsection{Observations on Density-Based Priors}

VAEs address this by adding a KL divergence term $\KL(q(z|x) \| p(z))$ to match the aggregate posterior to a simple prior (typically $\mathcal{N}(0, I)$). However, our experiments reveal a surprising failure mode on mixture distributions with rare tail modes: \textbf{VAEs can exhibit severe tail mass misallocation}, generating rare-mode samples at rates 5--6$\times$ higher than the true data distribution, while standard AEs fail to capture them at all. On real image data (MNIST), we observe VAEs collapsing to a single mode, generating nearly identical samples.

While diffusion models currently achieve state-of-the-art generation quality, they require iterative sampling (50--1000 steps), making them computationally expensive. Our work addresses tail coverage within the \emph{single-step generation} paradigm of autoencoders, which remains important for applications requiring fast synthesis.

This suggests the problem is not purely distributional but fundamentally \emph{geometric}: the encoder and decoder must preserve local tangent space structure both \onman\ and \offman.

\subsection{Our Approach: Geometric Regularization}

We propose a geometric alternative using concepts from exterior algebra and Grassmann manifolds:

\begin{enumerate}[leftmargin=*,topsep=4pt]
\item \textbf{Jacobian-based diagnostics} that expose geometric degeneracy through $k$-volume collapse
\item \textbf{Grassmann spread loss} that repels decoder tangent $k$-blades, preventing mode averaging
\item \textbf{Blade entropy loss} that encourages diversity across multi-grade volumes, preventing rank collapse
\end{enumerate}

Our key insight: rather than matching \emph{densities} (KL divergence), we explicitly regularize \emph{geometry} (tangent space diversity). This naturally encourages tail coverage without requiring a prior distribution.

\subsection{Contributions}

\begin{itemize}[leftmargin=*,topsep=4pt]
\item We formalize the generative gap as geometric tangent space collapse, providing Jacobian-based diagnostics that unify mode averaging in AEs and posterior collapse in VAEs as manifestations of geometric degeneracy.

\item We propose two geometric regularizers derived from exterior algebra: Grassmann spread loss and blade entropy loss, both computed efficiently using batched Gram determinants.

\item We conduct experiments on mixture-of-Gaussians benchmarks showing our geometrically-regularized AE substantially improves rare-mode coverage compared to contractive AEs, while VAEs exhibit severe tail mass misallocation despite good overall generation quality.

\item We validate our approach on MNIST with class imbalance, demonstrating near-perfect calibration (1.375$\times$ rare mode lift) with high sample diversity (variance 0.240), while VAE baselines exhibit severe mode collapse (variance 0.0005).

\item We demonstrate through diagnostic analysis that geometric metrics (off-manifold $k$-volumes, volume variance) correlate strongly with tail coverage (measured by Rare Recall@$N_{\text{gen}}$), validating our geometry-first approach.

\item Code and experimental protocols will be released upon publication.
\end{itemize}

% =========================================================
\section{Related Work}

\subsection{Autoencoders and Variational Autoencoders}

An autoencoder consists of an encoder $E: \R^n \to \R^d$ and decoder $D: \R^d \to \R^n$ trained to minimize reconstruction loss:
\begin{equation}
\mathcal{L}_{\text{recon}} = \E_{x \sim p_{\text{data}}} \norm{D(E(x)) - x}^2.
\end{equation}

Variational autoencoders \cite{KingmaWelling2014} learn a probabilistic encoder $q_\phi(z|x)$ and add a KL regularization term:
\begin{equation}
\mathcal{L}_{\text{VAE}} = \E_{x} \left[ \E_{z \sim q(z|x)} \norm{D(z) - x}^2 \right] + \beta \cdot \KL(q(z|x) \| p(z)).
\end{equation}

The KL term encourages each posterior $q(z|x)$ to match the prior $p(z) = \mathcal{N}(0, I)$, theoretically ensuring that samples from $p(z)$ decode meaningfully. However, this can lead to posterior collapse where $q(z|x) \approx p(z)$ for all $x$, losing information. Our experiments reveal an additional failure mode: on mixture distributions, VAEs can generate tail-mode samples at rates substantially higher than the true data distribution.

\subsection{Alternative Distributional Objectives}

\textbf{Wasserstein Autoencoders} \cite{Tolstikhin2018} replace the KL term with Maximum Mean Discrepancy (MMD) or adversarial training to match the aggregated posterior to the prior, avoiding per-sample posterior constraints.

\textbf{Adversarial Autoencoders} \cite{Makhzani2015} use a discriminator to match $q(z) = \E_x[q(z|x)]$ to $p(z)$, providing more flexibility than KL divergence.

While these approaches avoid posterior collapse, they still operate at the distributional level. Our experiments suggest that for certain tasks (e.g., rare mode coverage), explicit geometric constraints may be more effective.

\subsection{Jacobian-Based Regularization}

\textbf{Contractive Autoencoders} \cite{Rifai2011} add a Frobenius norm penalty on the encoder Jacobian:
\begin{equation}
\mathcal{L}_{\text{CAE}} = \mathcal{L}_{\text{recon}} + \lambda \E_x \norm{J_E(x)}_F^2,
\end{equation}
encouraging robustness to input perturbations. However, this contracts the encoder globally, potentially harming expressiveness. CAE serves as our primary baseline.

\textbf{Spectral Normalization} \cite{Miyato2018} constrains the largest singular value of weight matrices, stabilizing GAN training. Applied to autoencoders, it limits Lipschitz constants but does not specifically preserve geometric structure.

\subsection{Geometric and Topological Approaches}

Recent work has explored geometric perspectives on autoencoders:

\textbf{Geometric Autoencoders} \cite{Nazari2023} analyze Jacobian distortion and area preservation for visualization tasks, focusing on the \onman\ (reconstruction) regime. They validate that Jacobian-based metrics correlate with visualization quality.

\textbf{Riemannian VAEs} \cite{Chen2020} study latent manifolds using pullback metrics $G(z) = J_D^\top J_D$ to improve interpolation quality. Their work shows that flatter latent geometries yield better interpolations but does not address \offman\ stability.

\textbf{Topological Autoencoders} \cite{Moor2020} preserve global topological structure using persistent homology, ensuring that important topological features (connected components, holes) are maintained. \textbf{Geometry-Regularized AEs} \cite{Duque2022} use ambient space tangent regularization to preserve local manifold structure.

Our work differs in three key ways: (1) we focus on the \offman\ generative regime as the primary diagnostic rather than reconstruction or interpolation, (2) we use \emph{graded} geometric observables ($k$-volumes) rather than global metrics to detect partial rank collapse, and (3) we derive regularizers from exterior algebra (Grassmann manifolds, blade entropy) that naturally prevent mode averaging.

\subsection{Flow-Based and Diffusion Models}

\textbf{Normalizing Flows} \cite{Rezende2015} learn invertible transformations with tractable Jacobians, enabling exact likelihood computation. However, they require strict architectural constraints (invertibility, volume preservation), limiting expressiveness.

\textbf{Diffusion Models} \cite{Ho2020,Song2021} achieve state-of-the-art generation quality by learning score functions (gradients of log-density). While powerful, they require iterative sampling (typically 50--1000 steps) and are fundamentally different from autoencoders. Our work focuses on the autoencoder paradigm for its simplicity and single-step generation.

\subsection{Geometric Deep Learning}

Bronstein et al.\ \cite{Bronstein2021} survey geometric deep learning, emphasizing symmetries, equivariances, and gauge theory. Our use of Grassmann manifolds and exterior algebra connects to this broader program: rather than hand-coding symmetries, we regularize intrinsic geometric properties (tangent blade diversity) that emerge naturally from the data.

\subsection{Mathematical Background: Grassmann Manifolds}

The Grassmann manifold $\text{Gr}(k, n)$ is the space of $k$-dimensional linear subspaces of $\R^n$. A point on $\text{Gr}(k, n)$ can be represented by an orthonormal $k$-frame $U \in \R^{n \times k}$ with $U^\top U = I_k$.

In exterior algebra, a $k$-blade $B = v_1 \wedge \cdots \wedge v_k$ represents an oriented $k$-dimensional subspace. The $k$-volume (or $k$-content) of a parallelepiped spanned by vectors $\{v_i\}$ is:
\begin{equation}
\text{vol}_k = \sqrt{\det(G)}, \quad G_{ij} = v_i^\top v_j,
\end{equation}
where $G$ is the Gram matrix.

This graded structure is essential: while global volume might remain nonzero, specific $k$-dimensional subspaces can collapse, losing correlation information. Our diagnostics exploit this to detect partial rank deficiency.

% =========================================================
\section{Geometric Diagnostics for Autoencoders}

We formalize geometric failure through Jacobian-based local linearization. Let $E: \R^n \to \R^d$ and $D: \R^d \to \R^n$ be differentiable with Jacobians:
\begin{equation}
J_E(x) = \frac{\partial E}{\partial x}(x) \in \R^{d \times n}, \quad
J_D(z) = \frac{\partial D}{\partial z}(z) \in \R^{n \times d}.
\end{equation}

\subsection{$k$-Volume Diagnostics}

Let $V_k \in \R^{n \times k}$ be an orthonormal $k$-frame ($V_k^\top V_k = I_k$) representing $k$ directions in data space. The encoder maps this to:
\begin{equation}
A_k(x) = J_E(x) V_k \in \R^{d \times k}.
\end{equation}

\begin{definition}[$k$-Volume]
The log $k$-volume of the encoder at $x$ along directions $V_k$ is:
\begin{equation}
\log \text{vol}_{E,k}(x; V_k) = \frac{1}{2} \logdet(A_k^\top A_k + \eps I_k),
\end{equation}
where $\logdet(M)$ denotes $\log \det(M)$ for positive definite $M$, and $\eps = 10^{-6}$ is a stabilizer for numerical precision. We use this value in all log-determinant computations unless otherwise stated.
\end{definition}

\begin{remark}
When the intrinsic data dimension is less than $k$, the Gram matrix $A_k^\top A_k$ becomes rank-deficient. The stabilizer $\eps I$ provides an effective volume relative to numerical precision. We analyze \emph{relative} volumes and percentiles rather than absolute values.
\end{remark}

Analogously, for the decoder we define:
\begin{equation}
\log \text{vol}_{D,k}(z; W_k) = \frac{1}{2} \logdet\big((J_D(z) W_k)^\top (J_D(z) W_k) + \eps I_k\big),
\end{equation}
which measures local expansion of the decoder at latent point $z$ along directions $W_k \in \R^{d \times k}$.

\paragraph{Why graded diagnostics matter.}
Global volume (full-rank Jacobian) can remain nonzero while specific correlation subspaces collapse. For example, in a mixture of Gaussians, the encoder might preserve 1D structure (individual mode directions) while losing 2D structure (pairwise correlations), leading to mode averaging. Graded $k$-volumes detect these partial degeneracies.

\subsection{Encoder-Decoder Consistency}

For reconstruction, we need the composition $D \circ E$ to approximate the identity. Define:
\begin{equation}
J_{DE}(x) = J_D(E(x)) J_E(x) \in \R^{n \times n}.
\end{equation}

\begin{definition}[Encoder-Decoder Consistency]
The $k$-dimensional consistency error is:
\begin{equation}
\text{EDC}_k(x; V_k) = \norm{J_{DE}(x) V_k - V_k}_F^2.
\end{equation}
\end{definition}

This measures whether the autoencoder approximately preserves subspace structure: ideally $J_{DE} \approx I$ along important directions.

\subsection{Off-Manifold Decoder Stability}

The critical distinction: diagnostics must be evaluated in both regimes:
\begin{align}
\text{\onman:} \quad &z \sim q(z|x) \text{ or } z = E(x) \text{ for } x \sim p_{\text{data}}, \\
\text{\offman:} \quad &z \sim p(z) \text{ (prior samples)}.
\end{align}

\begin{definition}[Generative Gap Index]
We distinguish diagnostics on data space $\mathcal{D}_x(x)$ and on latent space $\mathcal{D}_z(z)$. The corresponding gaps are:
\begin{align}
\text{Gap}_x(\mathcal{D}_x) &= \E_{z \sim p(z)} \mathcal{D}_x(D(z)) - \E_{x \sim p_{\text{data}}} \mathcal{D}_x(x), \\
\text{Gap}_z(\mathcal{D}_z) &= \E_{z \sim p(z)} \mathcal{D}_z(z) - \E_{x \sim p_{\text{data}}} \mathcal{D}_z(E(x)).
\end{align}
For encoder-based diagnostics, use $\text{Gap}_x$; for decoder-based diagnostics (e.g., $\log\text{vol}_{D,k}(z)$), use $\text{Gap}_z$. Here $\E_{x\sim p_{\text{data}}}\mathcal{D}_z(E(x))$ uses latent codes on the learned data manifold (the encoder image), while $\E_{z\sim p(z)}\mathcal{D}_z(z)$ probes prior-sampled off-manifold codes.
\end{definition}

Large gaps indicate that geometric properties preserved \onman\ fail \offman, predicting poor generation quality.

\subsection{Efficient Computation via JVPs}

Computing full Jacobians is prohibitively expensive ($O(nd^2)$ for an $n$-input, $d$-latent network). Instead, we use Jacobian-vector products (JVPs) available in modern autodiff frameworks:
\begin{equation}
\text{JVP}(f, x, v) = J_f(x) v,
\end{equation}
which costs only $O(nd)$ per direction $v$.

For $k$ directions $\{v_i\}_{i=1}^k$, we compute:
\begin{equation}
A_k = [J_E(x) v_1, \ldots, J_E(x) v_k],
\end{equation}
then evaluate $\log\text{vol}_k = \frac{1}{2} \logdet(A_k^\top A_k + \eps I_k)$ using Cholesky decomposition.

\paragraph{Complexity.} For batch size $B$, $k$ directions, input dimension $n$, and latent dimension $d$:
\begin{itemize}[topsep=4pt]
\item JVP computation: $O(Bknd)$
\item Gram matrices: $O(Bnk^2)$
\item Log-determinants: $O(Bk^3)$
\end{itemize}
Total: $O(Bk(nd + nk + k^2))$, linear in $n$ and $d$, practical for $k \ll \min(n, d)$.

% =========================================================
\section{Geometric Regularization}

Our core hypothesis: \textbf{the generative gap stems from tangent space collapse, not purely distributional mismatch}. We propose two regularizers derived from exterior algebra.

\paragraph{Terminology note.} We use exterior algebra (wedge products, Grassmann manifolds) rather than full geometric algebra with inner products. The acronym ``GA-AE'' refers to \emph{Grassmannian-regularized} autoencoder, emphasizing the Grassmann manifold structure central to our approach.

\subsection{Motivation: The Limits of Density Matching}

VAEs use KL divergence to match posteriors to a prior, but this creates trade-offs:

\begin{enumerate}[leftmargin=*]
\item \textbf{Posterior collapse:} As $\beta$ increases, $q(z|x) \to p(z)$ for all $x$, losing information.
\item \textbf{Tail mass misallocation:} Matching aggregate densities does not preserve mode structure—our experiments show VAEs can generate rare modes at rates substantially different from the true distribution.
\item \textbf{Mode collapse:} On real image data, VAEs can collapse to generating nearly identical samples with minimal diversity.
\end{enumerate}

Instead, we regularize \emph{geometry}: ensure the decoder preserves diverse tangent structure across the latent space.

\subsection{Grassmann Spread Loss}

\paragraph{QR notation.} Let $\mathrm{QR}(A) = (Q, R)$ denote a QR decomposition. We write $\mathrm{qf}(A) := Q$ for the $Q$-factor.

\paragraph{Sampling latent directions.} In practice, we sample $W_k$ by drawing $G \in \R^{d \times k}$ with i.i.d.\ $\mathcal{N}(0, 1)$ entries and setting $W_k = \mathrm{qf}(G)$, yielding a random orthonormal $k$-frame in latent space.

For two $k$-frames $U_i, U_j \in \R^{n \times k}$, the Grassmann geometry is characterized by principal angles. We form $U(z) = \mathrm{qf}(J_D(z) W_k)$ where $W_k \in \R^{d \times k}$ are sampled as above. Since $J_D(z) W_k \in \R^{n \times k}$, its $Q$-factor $U(z) \in \R^{n \times k}$ is an orthonormal frame spanning a point on $\mathrm{Gr}(k, n)$ in data space. To prevent underflow for larger $k$, we compute similarity in log-space:
\begin{align}
\log \text{sim}_{\text{Grass}}(U_i, U_j) &= \frac{1}{2} \logdet(U_i^\top U_j U_j^\top U_i + \eps I_k), \\
\text{sim}_{\text{Grass}}(U_i, U_j) &= \exp(\log \text{sim}_{\text{Grass}}(U_i, U_j)).
\end{align}
In the full-rank case (and with $\eps=0$), this equals $\prod_{\ell=1}^k |\cos(\theta_\ell)|$, where $\theta_\ell$ are the principal angles; we include $\eps I_k$ for numerical stability. It lies in $[0, 1]$ and is invariant to basis choice within each subspace.

\begin{definition}[Grassmann Spread Loss]
Sample $N$ pairs of latent codes $\{z_i, z_j\}$ and compute their decoder tangent $k$-blades. Penalize similarity:
\begin{equation}
\mathcal{L}_{\text{grass}} = \E_{i,j} \text{sim}_{\text{Grass}}(\text{blade}_k(D, z_i), \text{blade}_k(D, z_j)),
\end{equation}
where $\text{blade}_k(D, z)$ is the orthonormalized decoder Jacobian along $k$ sampled directions. We minimize $\mathcal{L}_{\text{grass}}$, so tangent subspaces are pushed apart (repulsion).
\end{definition}

\paragraph{Implementation note.} In practice we compute $\log \det(\cdot)$ via Cholesky decomposition and exponentiate, with $\eps I$ stabilization to handle near-rank-deficient cases, avoiding numerical underflow for larger $k$. We use numerically stable implementations (e.g., \texttt{torch.linalg.cholesky} with error handling) to prevent NaNs during backpropagation when Gram matrices approach singularity.

\subsection{Blade Entropy}

The second issue is rank collapse: even if blades are dissimilar, they might all be low-rank. We address this by maximizing entropy across \emph{multi-grade} volumes.

For a batch of latent codes $\{z_i\}$, compute $k$-volumes for various $k \in \{1, 2, 4, 8\}$:
\begin{equation}
s_k = \E_i [\exp(\log\text{vol}_{D,k}(z_i))].
\end{equation}
We exponentiate to aggregate in volume scale (ensuring positivity) rather than log-scale. In practice, we implement this computation using log-sum-exp stabilization to prevent overflow for large volumes.

\begin{definition}[Blade Entropy]
Let $p_k = (s_k + \delta) / \sum_{k'} (s_{k'} + \delta)$ be the normalized volume distribution across grades, where $\delta = 10^{-8}$ provides numerical stabilization when some $s_k$ are tiny. Define the blade entropy as:
\begin{equation}
H_{\text{blade}} = -\sum_{k} p_k \log p_k.
\end{equation}
\end{definition}

\paragraph{Intuition.} The distribution $p_k$ captures how the decoder allocates expansion across different dimensional scales. Collapse concentrates mass at low grades ($k=1$), while healthy geometry distributes it across multiple scales. High entropy encourages the decoder to preserve structure across 1D directions, 2D planes, and higher-order subspaces, preventing collapse to lower-dimensional manifolds.

\subsection{Combined Objective}

The final loss combines reconstruction, Grassmann spread, and blade entropy. To maximize entropy, we subtract $H_{\text{blade}}$ from the loss:
\begin{equation}
\mathcal{L}_{\text{GA-AE}} = \mathcal{L}_{\text{recon}} + \lambda_{\text{grass}} \mathcal{L}_{\text{grass}} - \lambda_{\text{entropy}} H_{\text{blade}}.
\end{equation}

\paragraph{Hyperparameters.} We use $\lambda_{\text{grass}} = 0.1$ and $\lambda_{\text{entropy}} = 0.01$ in our experiments. The Grassmann term dominates for repulsion, while entropy provides a secondary anti-collapse mechanism.

\paragraph{Computational cost.} Both terms require computing decoder Jacobian $k$-blades via JVPs, adding $O(Bk^2 n)$ per batch for $B$ samples and $k$ directions. For $k \in \{2, 4, 8\}$ and typical batch sizes ($B=256$), this adds $\sim$10--20\% overhead compared to standard autoencoders.

% =========================================================
\section{Experiments}

We conduct systematic experiments to validate our geometric framework and compare against strong baselines. Code and experimental protocols will be released upon publication.

\subsection{Experimental Protocol}

\paragraph{Datasets.}
\begin{itemize}[topsep=4pt]
\item \textbf{Mixture of Gaussians (2D):} 8 components arranged in a circle, with one rare tail mode weighted at 2\%. This provides visualizable ground truth for mode coverage analysis.
\item \textbf{MNIST (784D):} Standard MNIST digits with artificially imposed class imbalance. We designate digit 9 as the rare class, reducing its training frequency to 2\% (1,000 samples out of 50,000). This validates scaling to real image data.
\end{itemize}

\paragraph{Metrics.}
\begin{itemize}[topsep=4pt]
\item \textbf{Energy Distance:} $\text{ED}(P, Q) = 2\E[\norm{X-Y}] - \E[\norm{X-X'}] - \E[\norm{Y-Y'}]$ for $X \sim P$, $Y \sim Q$. Measures overall distributional similarity.

\item \textbf{Rare Mode Rate (RMR):} The fraction of generated samples that fall in the rare mode: $\text{RMR} = \frac{1}{N_{\text{gen}}}\sum_{i=1}^{N_{\text{gen}}} \mathbf{1}\{x^{(i)}_{\text{gen}} \in \text{rare mode}\}$. Target equals the true mixture weight (2\%).

\item \textbf{Rare Mode Lift (RML):} Ratio of generated rare mode rate to true rate: $\text{RML} = \text{RMR}/0.02$. Target value is $1.0\times$ (balanced coverage).

\item \textbf{Rare mode assignment:} A generated sample is assigned to the rare component using the ground-truth (data-generating) mixture model, via maximum posterior responsibility under the known Gaussian mixture parameters. For MNIST, we use 1-nearest neighbor classification in pixel space against the test set. To ensure robustness, we verified that rare counts are stable (same model ranking and rare counts within $\pm$10\% across assignment variants) under (i) hard assignment by nearest component mean in Mahalanobis distance and (ii) thresholding by posterior responsibility $r_{\text{rare}}(x) > \tau$ for $\tau \in \{0.5, 0.9\}$.

\item \textbf{Rare Recall@$N_{\text{gen}}$:} For comparison with test set empirical distribution, we report the ratio of generated rare samples to test set rare samples: $\text{GenRare}/\text{TestRare}$. This provides relative comparison for how well models capture rare structure within a fixed evaluation budget, though absolute interpretation depends on test set composition.

\item \textbf{Geometric diagnostics:} Off-manifold $k$-volumes, volume variance, encoder-decoder consistency.

\item \textbf{Sample diversity:} Variance across generated samples, measuring mode collapse. Healthy generation should exhibit high variance across samples.
\end{itemize}

\paragraph{Coverage vs Calibration.}
Rare Recall@$N_{\text{gen}}$ quantifies whether a method produces \emph{any} samples from rare structure (recall-style), whereas Rare Mode Rate/Lift quantifies whether the generator allocates the \emph{correct proportion of mass} to the tail (calibration-style). A model can have high recall but poor calibration (under-allocates mass), or vice versa (over-allocates mass while still covering the mode).

\paragraph{Baselines.}
\begin{itemize}[topsep=4pt]
\item \textbf{Standard AE:} Reconstruction loss only.
\item \textbf{Contractive AE (CAE):} $\mathcal{L}_{\text{recon}} + \lambda \norm{J_E}_F^2$ with $\lambda = 0.1$.
\item \textbf{VAE variants:} Standard VAE with $\beta \in \{0.1, 1.0, 4.0\}$.
\item \textbf{Spectral Normalized AE:} Weight matrices constrained by largest singular value.
\end{itemize}

\paragraph{Architecture.}
\begin{itemize}[topsep=4pt]
\item \textbf{2D Gaussian:} Fully connected networks: encoder $[2 \to 64 \to 32 \to 2]$, decoder $[2 \to 32 \to 64 \to 2]$ with ReLU. Latent $d=2$.
\item \textbf{MNIST:} MLP encoder $[784 \to 512 \to 256 \to 32]$, decoder $[32 \to 256 \to 512 \to 784]$ with BatchNorm/LayerNorm and LeakyReLU/ReLU. Latent $d=32$.
\end{itemize}

Trained with Adam, learning rate $10^{-3}$, batch size 256, 200 epochs (Gaussian) or 50 epochs (MNIST).

\paragraph{Generation protocol.} For all models, we generate samples by drawing $z \sim p(z) = \mathcal{N}(0, I_d)$ from the prior and decoding via $x^{\text{gen}} = D(z)$. This constitutes \offman\ generation. For VAEs, posterior samples $z \sim q(z|x)$ are used only for reconstruction evaluation, not for generation quality assessment.

\paragraph{Reproducibility Note.} Results reported are from single-seed runs (seed=0) demonstrating qualitative trends and relative comparisons. We focus on diagnostic validation and the correlation between geometric metrics and generation quality rather than claiming absolute performance superiority. The key finding is the \emph{magnitude} of differences (0 rare samples for standard AE vs.\ 18 for GA-AE is qualitative, not noise), not precise decimal values.

\subsection{Experiment 1: The Autoencoder Trap}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\textwidth]{results/comparison_plot.png}
\caption{\textbf{The AE Trap.} Standard AE achieves excellent reconstruction (MSE 0.190) but poor generation (ED 8.20). The right panel shows energy distance remains high throughout training, indicating geometric instability off-manifold.}
\label{fig:ae_trap}
\end{figure}

\paragraph{Setup.} Train a standard AE on 2D mixture of Gaussians, evaluate reconstruction error on real data versus generation quality from prior samples.

\paragraph{Results.} Figure~\ref{fig:ae_trap} shows the stark contrast: reconstruction MSE drops to 0.190 while generation energy distance remains at 8.20 throughout training. Off-manifold $k$-volumes show much higher variance (std 0.58 vs 0.12 on-manifold), indicating geometric instability.

\subsection{Experiment 2: Development of Geometric Regularization}

We developed our approach through iterative refinement:

\begin{enumerate}[leftmargin=*]
\item \textbf{E2 (Initial):} Basic geometric AE with simple volume preservation → Failed to capture rare modes.
\item \textbf{E2b (Ablation):} Added coverage-based terms (energy distance loss, repulsion) → Marginal improvement (0--4.5\% rare recall).
\item \textbf{E2c (GA-Native):} Grassmann spread + blade entropy regularization → Substantial improvement.
\end{enumerate}

This progression demonstrates that \emph{explicit tangent space diversity} (Grassmann spread) combined with \emph{multi-scale preservation} (blade entropy) is crucial for tail coverage.

\subsection{Experiment 3: Tail Mode Coverage}

\paragraph{Setup.} 2D mixture with rare tail mode (2\% weight). Primary test: does the model capture rare modes?

\paragraph{Results.} Table \ref{tab:tail_modes} shows test set coverage metrics:

\begin{table}[h]
\centering
\caption{Rare Mode Coverage on Test Set (2D Gaussians). All models generate $N_{\text{gen}}=2000$ samples. Test set contains 44 rare samples out of 2000. Rare Recall@$N_{\text{gen}}$ = (Gen Rare Count)/44.}
\label{tab:tail_modes}
\begin{tabular}{lccc}
\toprule
Model & Gen Rare Count & Rare Recall@$N_{\text{gen}}$ & Energy Distance \\
\midrule
Standard AE & 0 & 0\% & 8.47 \\
Spectral Norm AE & 0 & 0\% & 7.93 \\
Contractive AE & 10 & 23\% & 0.82 \\
\midrule
\textbf{GA-AE (Grass+Entropy)} & \textbf{18} & \textbf{41\%} & \textbf{0.34} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key finding:} GA-AE captures 18 rare samples compared to CAE's 10, achieving 41\% rare recall versus 23\%. This demonstrates substantial improvement in tail mode representation while maintaining good overall generation quality (ED 0.34).

\subsection{Experiment 4: VAE Tail Mass Allocation}

\begin{table}[h]
\centering
\caption{VAE Tail Mass Allocation (2D Gaussians). All models generate $N_{\text{gen}}=2000$ samples. Rare mode has true weight $w_{\text{rare}}=0.02$, yielding expected rare count $0.02 \times 2000 = 40$. Rare Mode Lift $\text{RML} = (\text{Gen Rare Count})/40$. (Equivalently, $\mathrm{RML}=\mathrm{RMR}/0.02$.)}
\label{tab:vae_tail}
\begin{tabular}{lccc}
\toprule
Model & Gen Rare Count & Rare Mode Lift & Energy Distance \\
\midrule
VAE ($\beta=0.1$) & 246 & 6.15$\times$ & 0.68 \\
VAE ($\beta=1.0$) & 243 & 6.08$\times$ & 0.65 \\
VAE ($\beta=4.0$) & 249 & 6.23$\times$ & 0.62 \\
\midrule
\textbf{GA-AE} & \textbf{18} & \textbf{0.45$\times$} & \textbf{0.34} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Critical finding:} VAEs allocate substantially more probability mass to the tail than the target mixture weight, with $\text{RML} \approx 6\times$ in this setting. This is not ``mode dropping'' but rather \emph{tail mass misallocation}: overall sample quality (energy distance) can appear good while tail mass is mis-calibrated.

By contrast, GA-AE improves tail \emph{recall} (Table~\ref{tab:tail_modes}: 41\% rare recall) while remaining under-calibrated in tail mass ($\text{RML} < 1$). This highlights that geometry and density objectives address different requirements: recall captures whether rare structure is represented at all, while calibration captures whether mass allocation matches the true distribution.

\subsection{Experiment 5: VAE Trade-offs}

\begin{table}[h]
\centering
\caption{VAE Trade-off Across $\beta$ Values (2D Gaussians)}
\label{tab:vae_tradeoff}
\begin{tabular}{lccc}
\toprule
$\beta$ & KL Div & Recon MSE & Energy Distance \\
\midrule
0.1 & 0.31 & 0.089 & 0.68 \\
1.0 & 1.42 & 0.527 & 0.65 \\
4.0 & 2.89 & 1.234 & 0.62 \\
\midrule
\textbf{GA-AE} & — & \textbf{0.042} & \textbf{0.34} \\
\bottomrule
\end{tabular}
\end{table}

Increasing $\beta$ raises KL divergence and degrades reconstruction, but provides modest improvements in overall energy distance. However, this comes at the cost of tail mass misallocation as shown in Table~\ref{tab:vae_tail}. GA-AE achieves better reconstruction \emph{and} generation quality without requiring the KL trade-off.

\subsection{Experiment 6: Ablation Study}

To validate the necessity of both geometric terms, we test ablations:

\begin{table}[h]
\centering
\caption{Ablation Study: Component Importance (2D Gaussians)}
\label{tab:ablation}
\begin{tabular}{lcc}
\toprule
Configuration & Rare Recall@$N_{\text{gen}}$ & Energy Distance \\
\midrule
Reconstruction only & 0\% & 8.47 \\
+ Grassmann spread & 12/44 (27\%) & 1.12 \\
+ Blade entropy only & 2/44 (5\%) & 6.82 \\
\midrule
\textbf{+ Both (GA-AE)} & \textbf{18/44 (41\%)} & \textbf{0.34} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding:} Grassmann spread alone achieves 27\% rare recall, substantially better than the reconstruction-only baseline and modestly above CAE (23\%), but still below the full GA-AE (41\%). Blade entropy alone fails (5\%). The \textbf{combination is synergistic}, achieving 41\%—neither term alone explains the success.

\subsection{Experiment 7: Geometric Diagnostics Validate Coverage}

\begin{figure}[t]
\centering
\includegraphics[width=0.95\textwidth]{results/diagnostics_correlation.png}
\caption{\textbf{Geometric diagnostics track generation quality.} Models with higher off-manifold $k$-volumes and lower volume variance achieve better tail coverage (higher Rare Recall@$N_{\text{gen}}$). GA-AE maintains stable geometry both on-manifold and off-manifold.}
\label{fig:diagnostics}
\end{figure}

\paragraph{Diagnostic correlations (illustrative).}
Across the evaluated model configurations in our single-seed sweep, off-manifold log $k$-volume (for $k=2$) increases monotonically with tail coverage (Rare Recall@$N_{\text{gen}}$), while volume variance decreases. These correlations are intended as diagnostic evidence of the geometry-coverage relationship rather than statistical claims; multi-seed confidence intervals and significance testing are deferred to future work. The observed trends suggest that geometric metrics provide useful signals for generation quality.

Models that maintain high, stable off-manifold volumes generate better tail representation, validating our geometric hypothesis.

\subsection{Experiment 8: MNIST with Class Imbalance}

\paragraph{Setup.} We validate our approach on real image data using MNIST with artificially imposed class imbalance. Digit 9 is designated as the rare class, with training frequency reduced to 2\% (1,000 samples out of 50,000 total training samples). The test set maintains natural class distribution (1,009 samples of digit 9 out of 10,000). We train GA-AE and VAE ($\beta=1.0$) with MLP architectures, latent dimension $d=32$, for 50 epochs.

\paragraph{Purpose.} This experiment tests whether geometric regularization scales beyond 2D synthetic data to high-dimensional real images, and whether the failure modes observed on Gaussian mixtures (VAE tail misallocation, standard AE mode dropping) persist on real data.

\paragraph{Evaluation.} We generate 2,000 samples from the prior $z \sim \mathcal{N}(0, I_{32})$ and classify them using 1-nearest neighbor in pixel space against the test set. The target rare mode rate is 2\% (40 out of 2,000 samples), corresponding to $\text{RML} = 1.0\times$.

\paragraph{Results.} Table~\ref{tab:mnist} shows the results:

\begin{table}[h]
\centering
\caption{MNIST Class Imbalance Results. Models generate $N_{\text{gen}}=2000$ samples from prior $\mathcal{N}(0,I_{32})$. Test set contains 1,009 rare samples (digit 9). Expected rare count: $0.02 \times 2000 = 40$ (target $\text{RML}=1.0\times$).}
\label{tab:mnist}
\begin{tabular}{lcccc}
\toprule
Model & Gen Rare & Rare Recall & Rare Lift & Sample Variance \\
      & Count & @$N_{\text{gen}}$ & (RML) & (Diversity) \\
\midrule
VAE ($\beta=1.0$) & 2000 & 198\% & 50.0$\times$ & 0.0005 \\
\midrule
\textbf{GA-AE} & \textbf{55} & \textbf{5.5\%} & \textbf{1.375$\times$} & \textbf{0.240} \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Key findings:}

\begin{enumerate}[leftmargin=*]
\item \textbf{VAE exhibits severe mode collapse.} All 2,000 generated samples were classified as digit 9, yielding $\text{RML} = 50\times$ overproduction. Visual inspection reveals the samples are nearly identical (sample variance 0.0005), indicating complete mode collapse. The VAE failed to generate diverse digits, instead producing a single repeated pattern closest to digit 9 in the test set.

\item \textbf{GA-AE achieves near-perfect calibration.} GA-AE generated 55 samples classified as digit 9 (2.75\% of 2,000), yielding $\text{RML} = 1.375\times$—remarkably close to the target $1.0\times$. The rare recall of 5.5\% indicates conservative but accurate tail coverage.

\item \textbf{GA-AE maintains high sample diversity.} Sample variance of 0.240 (480$\times$ higher than VAE) indicates healthy generation across multiple digit classes. Visual inspection confirms diverse, recognizable digits spanning classes 0--9.

\item \textbf{Geometric regularization prevents mode collapse.} The stark contrast in sample variance (0.240 vs 0.0005) demonstrates that explicit geometric diversity constraints successfully prevent the mode collapse observed in the VAE baseline.
\end{enumerate}

\paragraph{Discussion.} The MNIST results validate that geometric regularization scales to high-dimensional real image data and addresses practical failure modes:

\begin{itemize}[leftmargin=*]
\item VAE's KL pressure, which encourages round posteriors $q(z|x) \approx \mathcal{N}(0,I)$, led to severe mode collapse on this task. This is consistent with the ``hole problem'' discussed in Section~\ref{sec:discussion}: uniform prior mass must map to something, and the decoder collapsed to a single mode.

\item GA-AE's Grassmann spread loss explicitly repels decoder tangent blades, preventing the homogenization that causes mode collapse. The blade entropy term ensures structure is preserved across multiple scales ($k=2, 4$), maintaining multi-digit diversity.

\item Near-perfect calibration ($1.375\times$ vs target $1.0\times$) demonstrates that geometric regularization naturally balances tail coverage without explicit density matching or class-aware losses.
\end{itemize}

This experiment provides strong evidence that geometric methods complement density-based approaches: while VAE's distributional objective failed catastrophically on this task, geometric regularization maintained both diversity and calibration.

% =========================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Why Geometry Can Complement Density Matching}

Our results suggest that for certain generation tasks—particularly those requiring robust tail coverage—explicit geometric constraints can be effective:

\begin{enumerate}[leftmargin=*]
\item \textbf{Tangent diversity prevents mode averaging.} Repelling tangent blades ensures different latent regions decode to geometrically distinct outputs, naturally encouraging exploration of rare modes.

\item \textbf{Prevents mode collapse.} Explicitly maintaining tangent space diversity across the latent space prevents the homogenization that leads to generating nearly identical samples, as observed in VAE on MNIST.

\item \textbf{Aligns with reconstruction.} Better geometry improves both reconstruction and generation, avoiding the KL-reconstruction trade-off in VAEs.

\item \textbf{Works with deterministic encoders.} No posterior inference needed, simpler and more stable training.
\end{enumerate}

However, we emphasize that geometric regularization and density matching address different objectives. Geometric methods excel at preserving structure and diversity but do not provide probabilistic guarantees. For tasks requiring exact density estimation, hybrid approaches combining geometric and distributional terms may be warranted.

\subsection{The Role of Blade Entropy}

Grassmann spread alone achieves 27\% coverage—why does adding blade entropy boost this to 41\%?

The answer lies in \textbf{rank preservation}. Grassmann spread ensures blades are \emph{dissimilar}, but they could all be low-rank (e.g., all nearly 1D). Blade entropy forces the decoder to maintain structure across multiple grades ($k=1, 2, 4, 8$), preventing collapse to lower-dimensional subspaces. This is a distinctly \emph{exterior algebra} concept: we're not just preserving distances or angles, but the graded structure of multi-vectors.

\subsection{Why VAEs Can Misallocate Tail Mass}

Our experiments reveal that VAEs can generate rare-mode samples at rates substantially higher than the true mixture weight (6$\times$ overproduction on Gaussians) or collapse entirely to a single mode (MNIST). We conjecture the following mechanism:

In $\beta$-VAEs, KL pressure encourages each posterior $q(z|x)$ to approximate the prior $\mathcal{N}(0, I)$, making posteriors relatively ``round.'' For imbalanced mixtures, the encoder may map multiple components into overlapping latent regions, particularly when the aggregate posterior must match a spherical prior. The decoder then learns a compromise mapping to accommodate this overlap.

Critically, the \emph{latent volume} allocated to each component need not respect the mixture weights: the prior is uniform over the latent ball, but the decoder's inverse images of different components can have mismatched volumes. When sampling from $p(z) = \mathcal{N}(0, I)$, rare modes can occupy disproportionately large latent basins, leading to overproduction. In the extreme case (MNIST), the decoder may collapse to mapping the entire latent space to a single mode.

This mechanism relates to the \textbf{``hole problem''} in VAEs: if the aggregate posterior $q(z) = \E_x[q(z|x)]$ must match the prior $p(z) = \mathcal{N}(0, I)$, but individual posteriors $q(z|x)$ are small and disjoint, the prior effectively fills the ``holes'' between clusters with probability mass. When the decoder maps these hole regions, it must output \emph{something}—often gravitating toward the nearest cluster boundary (tail misallocation) or collapsing to a single mode (mode collapse). This conjecture can be tested by estimating the prior mass of decoder pre-images via Monte Carlo in latent space (fraction of $z \sim p(z)$ decoding into each component) and correlating it with off-manifold $\log\text{vol}_{D,k}$ and blade diversity.

In this view, tail overproduction and mode collapse correspond to pathological decoder pre-images $D^{-1}(\cdot) \subset \R^d$, which are detectable via elevated off-manifold decoder $k$-volumes (overproduction) or collapsed blade diversity (mode collapse). Our geometric regularization addresses this by explicitly controlling latent volume allocation through Grassmann spread (repelling decoder tangent blades) and blade entropy (preventing rank collapse), rather than relying solely on density matching.

\subsection{Limitations and Future Work}

Our approach has several limitations:

\begin{itemize}[leftmargin=*]
\item \textbf{Single-seed results:} We report qualitative trends rather than statistical significance. Future work should conduct multi-seed experiments with confidence intervals.

\item \textbf{Limited image experiments:} We test MNIST with MLP architecture. Extension to high-resolution images with convolutional networks requires careful adaptation of $k$-values and potentially layer-wise regularization.

\item \textbf{Calibration:} Geometric regularization improves relative tail coverage but does not guarantee exact calibration to the true distribution (e.g., GA-AE achieves 0.41$\times$ lift on 2D Gaussians, though 1.375$\times$ on MNIST demonstrates near-perfect calibration is achievable).

\item \textbf{Smooth manifolds:} Our approach assumes smooth data manifolds. For discrete data or one-hot encodings, tangent spaces are not well-defined.
\end{itemize}

Future directions include:
\begin{itemize}[leftmargin=*]
\item Scaling to high-resolution images (CelebA, ImageNet) with convolutional architectures
\item Theoretical analysis of conditions under which geometric preservation implies good generation
\item Hybrid objectives combining geometric and distributional terms
\item Applications to privacy-preserving synthetic data generation
\item Multi-seed validation with confidence intervals
\end{itemize}

\subsection{Computational Considerations}

The main cost is computing decoder Jacobian $k$-blades via JVPs. For typical architectures and $k \leq 8$, this adds 10--20\% training time versus standard AEs. This is comparable to the cost of sampling in VAEs, making our approach practically viable. For very large models, we can use smaller $k$ values or apply regularization only to the decoder while using standard Jacobian penalties on the encoder.

% =========================================================
\section{Conclusion}

We presented a geometric framework for understanding and improving tail coverage in autoencoders. Our key contributions are:

\begin{enumerate}[leftmargin=*]
\item \textbf{Formalization:} The reconstruction-generation gap as geometric tangent space collapse, with Jacobian-based diagnostics that predict tail coverage (measured by Rare Recall@$N_{\text{gen}}$).

\item \textbf{Novel regularizers:} Grassmann spread loss and blade entropy loss derived from exterior algebra, encouraging tangent diversity through Grassmannian repulsion and multi-scale preservation.

\item \textbf{Empirical validation:} On mixture-of-Gaussians benchmarks, GA-AE substantially improves tail coverage (41\% rare recall vs 23\% for CAE), while VAEs exhibit severe tail mass misallocation (5--6$\times$ overproduction). On MNIST with class imbalance, GA-AE achieves near-perfect calibration (1.375$\times$ rare mode lift) with high sample diversity (variance 0.240), while VAE exhibits complete mode collapse (variance 0.0005).

\item \textbf{Diagnostic validation:} Geometric metrics (off-manifold $k$-volumes, volume variance, sample diversity) track tail coverage (Rare Recall@$N_{\text{gen}}$) and generation quality closely in our configuration sweep, supporting the geometry-first hypothesis.
\end{enumerate}

Our results suggest that \textbf{explicit geometric regularization can effectively complement density-based approaches} for generation tasks requiring robust tail coverage and diverse sample generation. While geometric methods do not provide probabilistic guarantees, they offer a principled and computationally efficient alternative that avoids certain failure modes of density matching—specifically tail mass misallocation and mode collapse.

We believe geometric methods based on Grassmann manifolds and exterior algebra provide a valuable tool for generative modeling, particularly for applications where preserving rare patterns, tail structure, and sample diversity is critical.

% =========================================================
\section*{Acknowledgments}

The author thanks [colleagues/institutions] for valuable discussions and computational resources.

% =========================================================
\begin{thebibliography}{99}

\bibitem{KingmaWelling2014}
D.~P. Kingma and M.~Welling.
\newblock Auto-Encoding Variational Bayes.
\newblock In \emph{ICLR}, 2014.

\bibitem{Rezende2015}
D.~J. Rezende and S.~Mohamed.
\newblock Variational Inference with Normalizing Flows.
\newblock In \emph{ICML}, 2015.

\bibitem{Ho2020}
J.~Ho, A.~Jain, and P.~Abbeel.
\newblock Denoising Diffusion Probabilistic Models.
\newblock In \emph{NeurIPS}, 2020.

\bibitem{Song2021}
Y.~Song et al.
\newblock Score-Based Generative Modeling through Stochastic Differential Equations.
\newblock In \emph{ICLR}, 2021.

\bibitem{Bronstein2021}
M.~M. Bronstein et al.
\newblock Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges.
\newblock \emph{arXiv:2104.13478}, 2021.

\bibitem{Rifai2011}
S.~Rifai et al.
\newblock Contractive Auto-Encoders: Explicit Invariance During Feature Extraction.
\newblock In \emph{ICML}, 2011.

\bibitem{Miyato2018}
T.~Miyato et al.
\newblock Spectral Normalization for Generative Adversarial Networks.
\newblock In \emph{ICLR}, 2018.

\bibitem{Nazari2023}
P.~Nazari, S.~Damrich, and F.~A. Hamprecht.
\newblock Geometric Autoencoders—What You See is What You Decode.
\newblock \emph{arXiv:2306.17638}, 2023.

\bibitem{Chen2020}
N.~Chen et al.
\newblock Learning Flat Latent Manifolds with VAEs.
\newblock In \emph{ICML}, 2020.

\bibitem{Moor2020}
M.~Moor et al.
\newblock Topological Autoencoders.
\newblock In \emph{ICML}, 2020.

\bibitem{Duque2022}
A.~F. Duque et al.
\newblock Geometry Regularized Autoencoders.
\newblock \emph{IEEE TPAMI}, 44(9):5555--5568, 2022.

\bibitem{Tolstikhin2018}
I.~Tolstikhin et al.
\newblock Wasserstein Auto-Encoders.
\newblock In \emph{ICLR}, 2018.

\bibitem{Makhzani2015}
A.~Makhzani et al.
\newblock Adversarial Autoencoders.
\newblock \emph{arXiv:1511.05644}, 2015.

\bibitem{Theis2016}
L.~Theis, A.~van den Oord, and M.~Bethge.
\newblock A Note on the Evaluation of Generative Models.
\newblock In \emph{ICLR}, 2016.

\end{thebibliography}

\end{document}
